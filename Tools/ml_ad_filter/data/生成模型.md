# AI 广告过滤模型训练指南

本指南介绍如何训练完整的两阶段 TensorFlow Lite 模型用于 AI 广告过滤功能。

## 模型架构

本项目使用**两阶段模型架构**：

1. **主题分析模型** (`topic_model.tflite`)：分析消息主题类型和意图
2. **广告分类模型** (`model.tflite`)：判断是否为广告

## 使用 Google Colab

### 步骤 1：打开 Colab

访问 https://colab.research.google.com/ 并新建笔记本

### 步骤 2：上传训练数据

将 `tools/ml_ad_filter/data/train.csv` 上传到 Colab

### 步骤 3：训练两阶段模型

```python
# 1. 安装依赖
!pip install tensorflow pandas scikit-learn -q

# 2. 上传数据
from google.colab import files
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import re

# 上传 train.csv
files.upload()

# 3. 加载训练数据
df = pd.read_csv('train.csv')
print(f"总样本数: {len(df)}")
print(f"广告样本: {len(df[df['label']=='ad'])}")
print(f"正常样本: {len(df[df['label']=='normal'])}")

# 4. 文本预处理（处理异型字）
def normalize_text(text):
    """标准化文本，处理异型字"""
    if pd.isna(text):
        return ""
    text = str(text)
    # 替换常见异型字
    replacements = {
        '薇': '微', '丄': '上', '沖': '冲', '値': '值',
        '玳': '代', '菠': '博', '菜': '彩',
        'Ⓑ': 'B', 'Ⓒ': 'C',
        'v.x': 'vx', 'V.X': 'VX', 'V X': 'VX',
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    # 移除零宽字符
    text = re.sub(r'[\u200b-\u200f\ufeff]', '', text)
    return text

df['text_normalized'] = df['text'].apply(normalize_text)

# ============================================
# 阶段 1：训练主题分析模型
# ============================================
print("\n" + "="*50)
print("阶段 1：训练主题分析模型")
print("="*50)

# 准备主题分类数据
topic_labels = df['topic'].unique()
topic_to_idx = {t: i for i, t in enumerate(topic_labels)}
idx_to_topic = {i: t for t, i in topic_to_idx.items()}

print(f"主题类别: {list(topic_labels)}")

# 分割数据
topic_y = [topic_to_idx[t] for t in df['topic'].tolist()]
X_train_topic, X_test_topic, y_train_topic, y_test_topic = train_test_split(
    df['text_normalized'].tolist(), topic_y, test_size=0.2, random_state=42, stratify=topic_y
)

# TF-IDF 特征提取（主题模型使用独立的vectorizer）
topic_vectorizer = TfidfVectorizer(max_features=8000, ngram_range=(1, 3))
X_train_topic_tfidf = topic_vectorizer.fit_transform(X_train_topic).toarray()
X_test_topic_tfidf = topic_vectorizer.transform(X_test_topic).toarray()

# 构建主题分类模型
topic_model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_topic_tfidf.shape[1],)),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(len(topic_labels), activation='softmax')
])

topic_model.compile(optimizer='adam',
                   loss='sparse_categorical_crossentropy',
                   metrics=['accuracy'])

# 训练
history = topic_model.fit(X_train_topic_tfidf, np.array(y_train_topic),
                         epochs=15, batch_size=32,
                         validation_data=(X_test_topic_tfidf, np.array(y_test_topic)),
                         verbose=1)

# 评估
loss, acc = topic_model.evaluate(X_test_topic_tfidf, np.array(y_test_topic))
print(f"主题模型测试集准确率: {acc:.4f}")

# 保存主题模型
converter = tf.lite.TFLiteConverter.from_keras_model(topic_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_topic_model = converter.convert()

with open('topic_model.tflite', 'wb') as f:
    f.write(tflite_topic_model)

with open('topic_labels.txt', 'w') as f:
    for i in range(len(topic_labels)):
        f.write(f"{idx_to_topic[i]}\n")

# 保存主题模型的 vectorizer 参数（用于 Android 端）
import json

# 保存词汇表
vocab = topic_vectorizer.vocabulary_
with open('topic_vocabulary.txt', 'w', encoding='utf-8') as f:
    for word, idx in sorted(vocab.items(), key=lambda x: x[1]):
        f.write(f"{word}\n")

# 保存 IDF 值
idf = topic_vectorizer.idf_
with open('topic_idf.txt', 'w') as f:
    for val in idf:
        f.write(f"{val}\n")

# 保存配置
config = {
    'max_features': topic_vectorizer.max_features,
    'ngram_range': list(topic_vectorizer.ngram_range),
    'vocab_size': len(vocab)
}
with open('topic_config.json', 'w') as f:
    json.dump(config, f, indent=2)

print("主题分析模型训练完成！")
print(f"词汇表大小: {len(vocab)}")
print(f"配置: {config}")

# ============================================
# 阶段 2：训练特征覆盖率模型（回归模型）
# ============================================
print("\n" + "="*50)
print("阶段 2：训练特征覆盖率模型（回归模型）")
print("="*50)

# 准备特征覆盖率数据（使用coverage_score列）
coverage_scores = df['coverage_score'].tolist()
X_train_cov, X_test_cov, y_train_cov, y_test_cov = train_test_split(
    df['text_normalized'].tolist(), coverage_scores, test_size=0.2, random_state=42
)

# TF-IDF 特征提取（覆盖率模型使用独立的vectorizer）
coverage_vectorizer = TfidfVectorizer(max_features=8000, ngram_range=(1, 3))
X_train_cov_tfidf = coverage_vectorizer.fit_transform(X_train_cov).toarray()
X_test_cov_tfidf = coverage_vectorizer.transform(X_test_cov).toarray()

# 构建特征覆盖率回归模型
coverage_model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_cov_tfidf.shape[1],)),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # 回归输出，范围0-1
])

coverage_model.compile(optimizer='adam',
                      loss='mse',  # 均方误差损失函数
                      metrics=['mae'])  # 平均绝对误差

# 训练
history = coverage_model.fit(X_train_cov_tfidf, np.array(y_train_cov),
                            epochs=15, batch_size=32,
                            validation_data=(X_test_cov_tfidf, np.array(y_test_cov)),
                            verbose=1)

# 评估
loss, mae = coverage_model.evaluate(X_test_cov_tfidf, np.array(y_test_cov))
print(f"覆盖率模型测试集MAE: {mae:.4f}")

# 保存覆盖率模型
converter = tf.lite.TFLiteConverter.from_keras_model(coverage_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_coverage_model = converter.convert()

with open('coverage_model.tflite', 'wb') as f:
    f.write(tflite_coverage_model)

# 保存覆盖率模型的 vectorizer 参数（用于 Android 端）
# 保存词汇表
vocab_coverage = coverage_vectorizer.vocabulary_
with open('coverage_vocabulary.txt', 'w', encoding='utf-8') as f:
    for word, idx in sorted(vocab_coverage.items(), key=lambda x: x[1]):
        f.write(f"{word}\n")

# 保存 IDF 值
idf_coverage = coverage_vectorizer.idf_
with open('coverage_idf.txt', 'w') as f:
    for val in idf_coverage:
        f.write(f"{val}\n")

# 保存配置
config_coverage = {
    'max_features': coverage_vectorizer.max_features,
    'ngram_range': list(coverage_vectorizer.ngram_range),
    'vocab_size': len(vocab_coverage),
    'model_type': 'regression',
    'output_range': '0-1'
}
with open('coverage_config.json', 'w') as f:
    json.dump(config_coverage, f, indent=2)

print("特征覆盖率模型训练完成！")
print(f"词汇表大小: {len(vocab_coverage)}")
print(f"配置: {config_coverage}")

# ============================================
# 下载所有模型文件
# ============================================
print("\n" + "="*50)
print("下载模型文件")
print("="*50)

files.download('topic_model.tflite')
files.download('topic_labels.txt')
files.download('topic_vocabulary.txt')
files.download('topic_idf.txt')
files.download('topic_config.json')
files.download('coverage_model.tflite')
files.download('coverage_vocabulary.txt')
files.download('coverage_idf.txt')
files.download('coverage_config.json')

print("所有文件已下载！")
print("\n部署到 Android 项目的文件列表：")
print("TMessagesProj/src/main/assets/ai_ad_filter/")
print("├── coverage_model.tflite    # 特征覆盖率回归模型")
print("├── coverage_vocabulary.txt  # 覆盖率模型词汇表")
print("├── coverage_idf.txt        # 覆盖率模型 IDF 值")
print("├── coverage_config.json    # 覆盖率模型配置")
print("├── topic_model.tflite      # 主题分析模型")
print("├── topic_labels.txt        # 主题标签")
print("├── topic_vocabulary.txt    # 主题模型词汇表")
print("└── topic_idf.txt          # 主题模型 IDF 值")
```

### 步骤 4：部署模型

将下载的文件放入以下目录：

```
TMessagesProj/src/main/assets/ai_ad_filter/
├── coverage_model.tflite      # 特征覆盖率回归模型
├── coverage_vocabulary.txt    # 覆盖率模型词汇表
├── coverage_idf.txt          # 覆盖率模型 IDF 值
├── coverage_config.json      # 覆盖率模型配置
├── topic_model.tflite        # 主题分析模型
├── topic_labels.txt          # 主题标签
├── topic_vocabulary.txt      # 主题模型词汇表
└── topic_idf.txt            # 主题模型 IDF 值
```

## 训练数据格式

### 基础格式

```csv
text,label,topic,intent,coverage_score
"专业上分服务，安全可靠，微信：bet888",ad,transaction,offer,0.95
"你好，最近怎么样？",normal,chat,inform,0.05
```

### 字段说明

- `text`: 消息文本内容
- `label`: 标签（`ad` 表示广告，`normal` 表示正常消息）
- `topic`: 主题类型（chat/consultation/promotion/transaction/recruitment/notification/greeting）
- `intent`: 意图类型（inform/request/offer/invite/urge）
- `coverage_score`: 特征覆盖率分数（0.0-1.0），广告样本0.8-1.0，正常样本0.0-0.2

### 异型字处理

训练数据包含以下类型的异型字样本：

1. **同音字替换**：薇信(微信)、丄分(上分)
2. **形近字替换**：沖值(充值)、値(值)
3. **字母替换**：ⒷⒸ(BC)、v.x(vx)
4. **分隔符**：博.彩、代.理
5. **空格分隔**：B o C、V X

## 模型配置

### 特征覆盖率模型参数（回归模型）

| 参数 | 值 | 说明 |
|------|-----|------|
| max_features | 8000 | TF-IDF 最大特征数 |
| ngram_range | (1, 3) | 使用 1-gram、2-gram、3-gram |
| 隐藏层 | 128 -> 64 -> 32 | 神经网络结构 |
| 输出层 | 1 (sigmoid) | 回归输出，范围0-1 |
| 损失函数 | MSE | 均方误差 |
| 评估指标 | MAE | 平均绝对误差 |
| epochs | 15 | 训练轮数 |
| batch_size | 32 | 批次大小 |
| dropout | 0.3-0.4 | 防止过拟合 |

### 主题分析模型参数

| 参数 | 值 | 说明 |
|------|-----|------|
| 输出类别 | 7 | chat/consultation/promotion/transaction/recruitment/notification/greeting |
| 其他参数 | 同上 | 与特征覆盖率模型相同 |

## 应用配置

### 阈值设置

在应用设置中可以调整AI广告特征覆盖率：

- **AI广告特征覆盖率**：默认 0.5
  - 范围：0.0 - 1.0
  - 值越高，过滤越严格，可能漏判
  - 值越低，过滤越宽松，可能误判
  - 建议值：0.5-0.7

### 主题分析开关

- 启用主题分析：默认开启
- 作用：先分析消息主题，再计算特征覆盖率
- 效果：提供更准确的上下文信息

### 过滤流程

1. 消息输入 → 主题分析模型总结主题
2. 主题内容 → 特征覆盖率模型计算覆盖率分数
3. 规则引擎辅助计算（基于关键词匹配）
4. 综合得分 = AI覆盖率 * 0.8 + 规则引擎得分 * 0.2
5. 综合得分 ≥ AI广告特征覆盖率阈值 → 判定为广告

## 故障排除

### 模型预测不准确

1. **增加训练数据**：特别是边界案例
2. **调整特征提取**：
   - 增加 `max_features` 到 10000
   - 调整 `ngram_range` 到 (1, 4)
3. **调整网络结构**：
   - 增加隐藏层神经元数量
   - 增加层数

### 异型字识别问题

1. 确保训练数据包含足够的异型字样本
2. 检查文本预处理函数是否正确
3. 考虑添加更多替换规则

### 模型文件过大

1. 减小 `max_features`
2. 使用更简单的网络结构
3. 启用更多优化选项：
   ```python
   converter.optimizations = [tf.lite.Optimize.DEFAULT]
   converter.target_spec.supported_types = [tf.float16]
   ```

### TensorFlow 版本不兼容

```bash
pip install tensorflow==2.14.0
```

## 性能优化建议

1. **数据增强**：对广告样本进行同义词替换、语序调整
2. **类别平衡**：确保正负样本比例均衡
3. **交叉验证**：使用 K-fold 交叉验证评估模型
4. **早停策略**：防止过拟合
   ```python
   early_stop = tf.keras.callbacks.EarlyStopping(
       monitor='val_loss', patience=3, restore_best_weights=True
   )
   ```

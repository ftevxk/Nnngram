# AI 广告关键词提取模型训练指南

本指南介绍如何训练 AI 关键词提取模型用于 AI 广告过滤功能

## 架构说明

系统包含两个核心数据文件，用途不同：

### 1. 训练数据 (train.csv)
用途：用于训练AI模型，让模型学习从文本中提取关键词
格式：text,label,keywords（3列）
- text: 消息文本内容
- label: ad(广告)或normal(正常)
- keywords: 关键词列表（用|分隔），标注这段文本包含哪些关键词

### 2. 特征库 (ad_feature_library.txt)
用途：Android应用运行时，用于判定提取到的关键词是否为广告及计算得分
格式：关键词,权重,频次,分类（4列）
- 关键词: 词本身
- 权重: 0.0-1.0，表示广告特征强度（训练后根据频次统计生成）
- 频次: 在训练数据中出现的次数
- 分类: ad(广告)或normal(正常)

## 使用 Google Colab

### 步骤 1：打开 Colab

访问 https://colab.research.google.com/ 并新建笔记本

### 步骤 2：上传训练数据

将 `tools/ml_ad_filter/data/train.csv` 上传到 Colab

### 步骤 3：训练关键词提取模型

```python
# 1. 安装依赖
!pip install tensorflow pandas scikit-learn jieba -q

# 2. 上传数据
from google.colab import files
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import json
import jieba

# 上传 train.csv
files.upload()

# 3. 加载训练数据
df = pd.read_csv('train.csv')
print(f"总样本数: {len(df)}")
print(f"广告样本: {len(df[df['label']=='ad'])}")
print(f"正常样本: {len(df[df['label']=='normal'])}")

# 4. 文本预处理（处理异型字和表情符号）
def normalize_text(text):
    # 标准化文本，处理异型字和表情符号
    if pd.isna(text):
        return ""
    text = str(text)
    
    # 替换常见异型字
    replacements = {
        '薇': '微', '丄': '上', '沖': '冲', '値': '值',
        '玳': '代', '菠': '博', '菜': '彩',
        'Ⓑ': 'B', 'Ⓒ': 'C',
        'v.x': 'vx', 'V.X': 'VX', 'V X': 'VX',
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    
    # 移除零宽字符
    text = re.sub(r'[\u200b-\u200f\ufeff]', '', text)
    
    # 移除表情符号
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # 表情符号
        u"\U0001F300-\U0001F5FF"  # 符号和象形文字
        u"\U0001F680-\U0001F6FF"  # 交通和地图符号
        u"\U0001F1E0-\U0001F1FF"  # 国旗
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)
    
    # 标准化分隔符
    text = re.sub(r'[-—\s]+', ' ', text)
    
    return text

df['text_normalized'] = df['text'].apply(normalize_text)

# 5. 使用jieba进行中文分词
def jieba_tokenize(text):
    # 使用jieba进行中文分词
    words = jieba.lcut(text)
    # 过滤掉空格和标点
    words = [w.strip() for w in words if w.strip() and not re.match(r'^\W+$', w)]
    return words

# 6. 解析关键词标签
def parse_keywords(keywords_str):
    # 解析关键词字符串，返回关键词列表（不含权重和频次）
    if pd.isna(keywords_str) or keywords_str == '':
        return []
    return [k.strip() for k in keywords_str.split('|') if k.strip()]

df['keywords_list'] = df['keywords'].apply(parse_keywords)

# ============================================
# 训练关键词提取模型
# ============================================
print("\n" + "="*50)
print("训练关键词提取模型")
print("="*50)

# 准备训练数据 - 从keywords列提取所有唯一关键词
all_keywords = set()
for keywords in df['keywords_list']:
    all_keywords.update(keywords)
all_keywords = sorted(list(all_keywords))
keyword_to_idx = {k: i for i, k in enumerate(all_keywords)}
idx_to_keyword = {i: k for k, i in keyword_to_idx.items()}

print(f"关键词总数: {len(all_keywords)}")
print(f"关键词列表: {all_keywords[:20]}...")  # 显示前20个

# 构建多标签目标（每个样本对应一个多标签向量）
y = np.zeros((len(df), len(all_keywords)))
for i, keywords in enumerate(df['keywords_list']):
    for kw in keywords:
        if kw in keyword_to_idx:
            y[i, keyword_to_idx[kw]] = 1.0

# 分割数据
X_train, X_test, y_train, y_test = train_test_split(
    df['text_normalized'].tolist(), y, test_size=0.2, random_state=42
)

# 使用jieba分词的自定义tokenizer
def custom_tokenizer(text):
    return jieba_tokenize(text)

# TF-IDF 特征提取
vectorizer = TfidfVectorizer(
    max_features=15000,  # 增加特征数以提高覆盖率
    ngram_range=(1, 3),
    tokenizer=custom_tokenizer,
    token_pattern=None  # 使用自定义tokenizer
)
X_train_tfidf = vectorizer.fit_transform(X_train).toarray()
X_test_tfidf = vectorizer.transform(X_test).toarray()

# 构建关键词提取模型（多标签分类）
model = tf.keras.Sequential([
    tf.keras.layers.Dense(512, activation='relu', input_shape=(X_train_tfidf.shape[1],)),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(len(all_keywords), activation='sigmoid')  # 多标签输出
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# 训练
history = model.fit(X_train_tfidf, y_train,
                   epochs=30, batch_size=32,
                   validation_data=(X_test_tfidf, y_test),
                   verbose=1)

# 评估
loss, acc = model.evaluate(X_test_tfidf, y_test)
print(f"模型测试集准确率: {acc:.4f}")

# 保存模型
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

with open('keyword_extractor_model.tflite', 'wb') as f:
    f.write(tflite_model)

# 保存关键词列表
with open('keyword_extractor_labels.txt', 'w', encoding='utf-8') as f:
    for kw in all_keywords:
        f.write(f"{kw}\n")

# 保存词汇表
vocab = vectorizer.vocabulary_
with open('keyword_extractor_vocab.txt', 'w', encoding='utf-8') as f:
    for word, idx in sorted(vocab.items(), key=lambda x: x[1]):
        f.write(f"{word}\n")

# 保存 IDF 值
idf = vectorizer.idf_
with open('keyword_extractor_idf.txt', 'w') as f:
    for val in idf:
        f.write(f"{val}\n")

# 保存配置
config = {
    'max_features': vectorizer.max_features,
    'ngram_range': list(vectorizer.ngram_range),
    'vocab_size': len(vocab),
    'keyword_count': len(all_keywords),
    'model_type': 'multi_label_classification',
    'tokenizer': 'jieba'
}
with open('keyword_extractor_config.json', 'w') as f:
    json.dump(config, f, indent=2)

print("关键词提取模型训练完成！")
print(f"词汇表大小: {len(vocab)}")
print(f"关键词数量: {len(all_keywords)}")
print(f"配置: {config}")

# ============================================
# 生成特征库（用于Android端运行时判定）
# 基于训练数据统计关键词权重和频次
# ============================================
print("\n" + "="*50)
print("生成特征库")
print("="*50)

# 统计每个关键词在广告样本中的出现频次
keyword_stats = {}
for i, row in df.iterrows():
    if row['label'] == 'ad':
        for kw in row['keywords_list']:
            if kw not in keyword_stats:
                keyword_stats[kw] = 0
            keyword_stats[kw] += 1

# 计算权重（基于出现频次，频次越高权重越高）
max_count = max(keyword_stats.values()) if keyword_stats else 1

with open('ad_feature_library.txt', 'w', encoding='utf-8') as f:
    f.write("# AI广告关键词特征库\n")
    f.write("# 格式：关键词,权重,频次,分类\n")
    f.write("# 权重范围：0.0-1.0\n")
    f.write("# 分类：ad(广告), normal(正常)\n")
    f.write("# 说明：此文件用于Android端运行时判定广告得分\n")
    f.write("\n")

    for kw, count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True):
        # 根据频次计算权重：基础0.5 + (频次/最大频次) * 0.45，最高0.95
        weight = min(0.95, 0.5 + (count / max_count) * 0.45)
        f.write(f"{kw},{weight:.2f},{count},ad\n")

print(f"特征库已生成，包含 {len(keyword_stats)} 个关键词")
print("特征库格式：关键词,权重,频次,分类")

# ============================================
# 下载所有模型文件
# ============================================
print("\n" + "="*50)
print("下载模型文件")
print("="*50)

files.download('keyword_extractor_model.tflite')
files.download('keyword_extractor_labels.txt')
files.download('keyword_extractor_vocab.txt')
files.download('keyword_extractor_idf.txt')
files.download('keyword_extractor_config.json')
files.download('ad_feature_library.txt')

print("所有文件已下载！")
print("\n部署到 Android 项目的文件列表：")
print("TMessagesProj/src/main/assets/ai_ad_filter/")
print("├── keyword_extractor_model.tflite    # 关键词提取模型")
print("├── keyword_extractor_labels.txt      # 关键词标签")
print("├── keyword_extractor_vocab.txt       # 词汇表")
print("├── keyword_extractor_idf.txt        # IDF值")
print("└── ad_feature_library.txt           # 特征库（运行时判定用）")
```

### 步骤 4：部署模型

将下载的文件放入以下目录：

```
TMessagesProj/src/main/assets/ai_ad_filter/
├── keyword_extractor_model.tflite      # 关键词提取模型
├── keyword_extractor_labels.txt        # 关键词标签
├── keyword_extractor_vocab.txt         # 词汇表
├── keyword_extractor_idf.txt          # IDF值
├── keyword_extractor_config.json      # 模型配置
└── ad_feature_library.txt             # 特征库（运行时判定用）
```

## 数据文件格式详解

### 训练数据格式 (train.csv)

用于训练AI模型，格式为3列：

```csv
text,label,keywords
"专业上分服务，安全可靠，微信：bet888",ad,"上分|微信|博彩"
"你好，最近怎么样？",normal,""
"跑分兼职，日赚500-2000",ad,"跑分|兼职|赚钱"
```

**字段说明**：
- `text`: 消息文本内容
- `label`: 标签（`ad` 表示广告，`normal` 表示正常消息）
- `keywords`: 关键词列表，用 `|` 分隔。只包含关键词名称，**不包含权重和频次**

**关键词标注规范**：
1. 核心广告词：博彩、跑分、上分、下分、刷单、返利等
2. 交易相关词：充值、收款码、代收、代付、USDT等
3. 招募相关词：代理、招募、加盟、兼职、月入过万等
4. 新用户推广词：新用户、注册、首存、送、邀请、扶持等
5. 联系方式词：微信、QQ、私聊、联系等

### 特征库格式 (ad_feature_library.txt)

用于Android端运行时判定，格式为4列：

```
# AI广告关键词特征库
# 格式：关键词,权重,频次,分类
# 权重范围：0.0-1.0
# 分类：ad(广告), normal(正常)

博彩,0.95,16,ad
跑分,0.92,12,ad
上分,0.90,10,ad
新用户,0.70,8,ad
注册,0.70,15,ad
```

**字段说明**：
- `关键词`: 词本身
- `权重`: 0.0-1.0，表示广告特征强度（训练后根据频次统计生成）
- `频次`: 在训练数据中出现的次数
- `分类`: ad(广告)或normal(正常)

**权重计算方式**：
```python
weight = min(0.95, 0.5 + (count / max_count) * 0.45)
```
基础权重0.5，根据频次占比增加，最高0.95

## 异型字和干扰处理

训练数据应包含以下类型的样本：

1. 同音字替换：薇信(微信)、丄分(上分)
2. 形近字替换：沖值(充值)、値(值)
3. 字母替换：ⒷⒸ(BC)、v.x(vx)
4. 分隔符：博.彩、代.理
5. 空格分隔：B o C、V X
6. 表情符号：🥰😂😀等干扰

## 模型配置

### 关键词提取模型参数

| 参数 | 值 | 说明 |
|------|-----|------|
| max_features | 15000 | TF-IDF 最大特征数，增加以提高覆盖率 |
| ngram_range | (1, 3) | 使用 1-gram、2-gram、3-gram |
| tokenizer | jieba | 使用jieba中文分词 |
| 隐藏层 | 512 -> 256 -> 128 -> 64 | 神经网络结构 |
| 输出层 | N (sigmoid) | 多标签输出，N为关键词数量 |
| 损失函数 | Binary Crossentropy | 二分类交叉熵 |
| epochs | 30 | 训练轮数 |
| batch_size | 32 | 批次大小 |
| dropout | 0.3-0.4 | 防止过拟合 |

## 应用配置

### 阈值设置

在应用设置中可以调整AI广告判定阈值：

- AI广告判定阈值：默认 0.65
  - 范围：0.0 - 1.0
  - 值越高，过滤越严格，可能漏判
  - 值越低，过滤越宽松，可能误判
  - 建议值：0.6-0.7

### 过滤流程

1. 消息输入 → 文本预处理（去除表情符号、标准化）
2. jieba分词 → AI关键词提取模型提取关键词及频次
3. 提取的关键词与特征库比对，获取权重
4. 计算得分 = 概率并集公式 P = 1 - ∏(1 - wi)
5. 得分 ≥ 阈值 → 判定为广告

### 用户反馈机制

用户可以通过长按消息选择"AI关键词特征提取"：
1. AI模型分析消息内容，提取潜在广告关键词
2. 显示提取结果，用户选择要添加的关键词
3. 选中的关键词加入特征库

## 故障排除

### 模型预测不准确

1. 增加训练数据：特别是边界案例和表情符号干扰案例
2. 调整特征提取：
   - 增加 `max_features` 到 20000
   - 调整 `ngram_range` 到 (1, 4)
3. 调整网络结构：
   - 增加隐藏层神经元数量
   - 增加层数

### 关键词提取不完整

1. 确保训练数据中的关键词标注完整
2. 增加关键词覆盖范围（特别是新用户推广类）
3. 调整模型输出阈值（降低到0.3）
4. 检查jieba分词词典是否需要自定义

### 模型文件过大

1. 减小 `max_features`
2. 减少关键词数量（只保留高频关键词）
3. 使用更简单的网络结构
4. 启用更多优化选项：
   ```python
   converter.optimizations = [tf.lite.Optimize.DEFAULT]
   converter.target_spec.supported_types = [tf.float16]
   ```

### TensorFlow 版本不兼容

```bash
pip install tensorflow==2.14.0
```

## 性能优化建议

1. 数据增强：对广告样本进行同义词替换、语序调整、表情符号添加
2. 类别平衡：确保正负样本比例均衡
3. 交叉验证：使用 K-fold 交叉验证评估模型
4. 早停策略：防止过拟合
   ```python
   early_stop = tf.keras.callbacks.EarlyStopping(
       monitor='val_loss', patience=3, restore_best_weights=True
   )
   ```
5. 学习率调度：使用学习率衰减策略
   ```python
   lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
       monitor='val_loss', factor=0.5, patience=2
   )
   ```
